use super::commit_time_histo::CommitTimeHisto;
use super::kwc::{KeywordCounter, KeywordCounterSet};
use super::tech::{Tech, TechHistory};
use super::ProjectReportOverview;
use crate::graphql::RustScalarValue;
use crate::utils::sha256::hash_str_to_sha256_as_base58;
use crate::{contributor::Contributor, git::GitLogEntry, utils};
use chrono::{DateTime, Utc};
use flate2::write::GzEncoder;
use flate2::Compression;
use juniper::GraphQLObject;
use path_absolutize::{self, Absolutize};
use serde::{Deserialize, Serialize};
use serde_json;
use std::collections::{HashMap, HashSet};
use std::fs::File;
use std::io::prelude::*;
use std::path::{Path, PathBuf};
use tracing::{debug, error, info, warn};

/// Contains the number of elements per list to help with DB queries.
/// The numbers are calculated once before saving the Report in the DB.
#[derive(Serialize, Deserialize, Clone, Debug, GraphQLObject)]
#[graphql(scalar = RustScalarValue)]
pub struct ListCounts {
    tech: u64,
    contributor_git_ids: u64,
    per_file_tech: u64,
    file_types: u64,
    reports_included: u64,
    projects_included: u64,
    git_ids_included: u64,
    contributors: u64,
    tree_files: u64,
    recent_project_commits: u64,
    keywords: u64,
}

#[derive(Serialize, Deserialize, Clone, Debug, GraphQLObject)]
#[graphql(scalar = RustScalarValue)]
#[serde(rename = "tech")]
pub struct Report {
    /// The exact timestamp of the report generation in ISO3389 format.
    /// E.g. 2018-12-09T22:29:40+01:00
    pub timestamp: String,
    /// Member email address for STM notifications. No update is needed if None. Clear if Some("").
    /// Only used for Inbox reports.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub primary_email: Option<String>,
    /// GitHub user name, if known
    #[serde(skip_serializing_if = "Option::is_none")]
    pub github_user_name: Option<String>,
    /// A public name of the project, if known. GitHub project names do not include the user name.
    /// E.g. `https://github.com/awslabs/aws-lambda-rust-runtime.git` would be `aws-lambda-rust-runtime`.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub github_repo_name: Option<String>,
    /// A unique identifier of the dev on STM server, if known.
    /// Populated by the server upon report submission and is None otherwise.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub owner_id: Option<String>,
    //// A unique identifier of the dev on STM server, if known.
    /// Populated by the server upon report submission and is None otherwise.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub project_id: Option<String>,
    /// A UUID of the report
    #[serde(skip_serializing_if = "String::is_empty", default = "String::new")]
    pub report_id: String,
    /// A unique name containing user name and project name when stored in S3, e.g. `rimutaka/stackmuncher.report`
    #[serde(skip_serializing_if = "String::is_empty", default = "String::new")]
    pub report_s3_name: String,
    /// A GH ownership validation Gist ID.
    /// E.g. fb8fc0f87ee78231f064131022c8154a from https://gist.github.com/rimutaka/fb8fc0f87ee78231f064131022c8154a
    /// The app validates it, but it has to be re-checked on the server on changes and possibly more often.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub gh_validation_id: Option<String>,
    /// The very last commit at the time of the report generation.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub report_commit_sha1: Option<String>,
    /// A SHA1 hash of all commit SHA1s to determine changes by looking at the log
    #[serde(skip_serializing_if = "Option::is_none")]
    pub log_hash: Option<String>,
    /// Is `true` if the report was generated by adding a single commit to a cached report
    #[serde(default = "default_as_false")]
    pub is_single_commit: bool,
    /// Git identity of the author of the last (HEAD) commit. Should only be present in the project report.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub last_commit_author: Option<String>,
    /// SHA1 of the last commit made by the contributor. Used in contributor reports only and is blank in project reports.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub last_contributor_commit_sha1: Option<String>,
    /// An ISO formatted date of the last commit made by the contributor.
    /// Used in contributor reports only and is blank in project reports.
    /// E.g. 2018-12-09T22:29:40+01:00
    #[serde(skip_serializing_if = "Option::is_none")]
    pub last_contributor_commit_date_iso: Option<String>,
    /// An EPOCH formatted date of the last commit made by the contributor.
    /// Used in contributor reports only and is blank in project reports.
    /// E.g. 1627176058
    #[serde(skip_serializing_if = "Option::is_none")]
    pub last_contributor_commit_date_epoch: Option<i64>,
    /// SHA1 of the first commit made by the contributor.
    /// Used in contributor reports only and is blank in project reports.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub first_contributor_commit_sha1: Option<String>,
    /// An ISO formatted date of the first commit made by the contributor.
    /// Used in contributor reports only and is blank in project reports.
    /// E.g. 2018-12-09T22:29:40+01:00
    #[serde(skip_serializing_if = "Option::is_none")]
    pub first_contributor_commit_date_iso: Option<String>,
    /// An EPOCH formatted date of the first commit made by the contributor.
    /// Used in contributor reports only and is blank in project reports.
    /// E.g. 1627176058
    #[serde(skip_serializing_if = "Option::is_none")]
    pub first_contributor_commit_date_epoch: Option<i64>,
    /// The date of the first commit for the entire repo.
    /// Combined reports have  the same value as first_contributor_commit.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub date_init: Option<String>,
    /// The date of the last commit for the entire repo or the current HEAD.
    /// Combined reports have  the same value as last_contributor_commit.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub date_head: Option<String>,
    /// The number of records in `contributor_git_ids` for contributor report.
    /// Contributors only have their own contributor ID included, so it is not possible to say how big the team
    /// was just by looking at the contributor report.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub contributor_count: Option<u64>,
    /// Lines Of Code (excludes blank lines) to show the size of the project.
    /// The value is set to the size of the project in project and contributor reports.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub loc_project: Option<u64>,
    /// Total number of unique library names to show the breadth of the project.
    /// The value is set to the size of the project in project and contributor reports.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub libs_project: Option<u64>,
    /// Total number of commits by the contributor, if there is one. Valid for contributor reports only.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub commit_count_contributor: Option<u64>,
    /// Total number of commits in the repo. Valid for repo and contributor reports.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub commit_count_project: Option<u64>,
    /// List of names or emails of all project contributors (authors and committers) from `contributors` section.
    /// This member is only set on project reports and is missing from individual or combined contributor reports.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub contributor_git_ids: Option<HashSet<String>>,
    /// Contains the number of elements per list contained in this report to help with DB queries.
    /// The values are calculated once before saving the reports.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub list_counts: Option<ListCounts>,
    /// Combined summary per technology, e.g. Rust, C# or CSS
    /// This member can be shared publicly after some clean up
    pub tech: HashSet<Tech>,
    /// Per-file technology summary, e.g. Rust/main.rs.
    /// This member should not be shared publicly, unless it's a public project
    /// because file names are sensitive info that can be exploited.
    #[serde(skip_serializing_if = "HashSet::is_empty", default = "HashSet::new")]
    pub per_file_tech: HashSet<Tech>,
    #[serde(skip_serializing_if = "HashSet::is_empty", default = "HashSet::new")]
    pub unprocessed_file_names: HashSet<String>,
    /// A list of all file extensions used in the project with the number of times they were encountered.
    #[serde(skip_serializing_if = "HashSet::is_empty", default = "HashSet::new")]
    pub file_types: HashSet<KeywordCounter>,
    /// S3 keys of the reports from `report_s3_name` merged into a combined user or org report
    /// This attribute was depricated in favour of projects_included, but has to be in use until
    /// https://github.com/stackmuncher/stm-html/issues/8 is resolved.
    #[serde(skip_serializing_if = "HashSet::is_empty", default = "HashSet::new")]
    pub reports_included: HashSet<String>,
    // Brief details about the projects included into a combined user or org report.
    /// Blank for individual project reports. It is only needed by STM server to display project details on the combined report page
    /// without going to the individual project reports.
    #[serde(skip_serializing_if = "Vec::is_empty", default = "Vec::new")]
    pub projects_included: Vec<ProjectReportOverview>,
    /// A list of GIT identities for the contributors included in the report.
    /// Used only in combined contributor reports
    #[serde(skip_serializing_if = "HashSet::is_empty", default = "HashSet::new")]
    pub git_ids_included: HashSet<String>,
    /// List of names and emails of all committers for this repo. Only applies to per-project reports.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub contributors: Option<Vec<Contributor>>,
    /// Number of commits per UTC hour and other stats related to committer active hours.
    /// Used to determine approximate active timezone of the dev.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub commit_time_histo: Option<CommitTimeHisto>,
    /// The current list of files in the GIT tree
    #[serde(skip_serializing_if = "Option::is_none")]
    pub tree_files: Option<HashSet<String>>,
    /// The last N commits for matching projects that changed name, remote URL or any other identifying property
    /// The commits are shortened and joined with their EPOCHs in a single string. E.g. `e29d17e6_1627380297`
    #[serde(skip_serializing_if = "Option::is_none")]
    pub recent_project_commits: Option<Vec<String>>,
    /// A unique list of all keywords found in the report for search. Normalized to lower case and sorted a-z.
    /// Populated during merge.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub keywords: Option<HashSet<String>>,
}

/// A plug for Serde default
fn default_as_false() -> bool {
    false
}

impl Report {
    /// .report
    pub const REPORT_FILE_NAME_SUFFIX: &'static str = ".report";

    /// Repos with this more files than this are ignored
    /// This is a temporary measure. The file count should be taken after some files were ignored,
    /// but since ignoring files like nodejs modules is not implemented we'll just ignore such repos.
    pub const MAX_FILES_PER_REPO: u64 = 10000;

    /// All project reports created prior to this date must be reprocessed
    pub const REPORT_FORMAT_VERSION: &'static str = "2021-11-02T00:23:00+00:00";

    /// Adds up `tech` totals from `other_report` into `self`, clears unprocessed files and unknown extensions.
    pub fn merge(merge_into: Option<Self>, other_report: Self) -> Option<Self> {
        let mut merge_into = merge_into;
        let mut other_report = other_report;

        // prepare an overview of the project being merged into the combined report
        // before `other_report` gets pulled to pieces by the merge
        let other_report_overview = other_report.get_overview();

        // check if there is anything useful in the other report
        if other_report_overview.loc == 0 {
            warn!("LoC=0 in the report to merge. Skipping.");
            return merge_into;
        }

        // update keyword summaries and muncher name in all tech records
        let mut new_rep_tech = Report::new();
        for mut tech in other_report.tech.drain() {
            tech.refs_kw = Tech::new_kw_summary(&tech.refs);
            tech.pkgs_kw = Tech::new_kw_summary(&tech.pkgs);
            // reset the muncher names on other_report to merge per-language
            // tech1==tech2 if munchers and languages are the same
            // we want to combine multiple munchers for the same language
            tech.muncher_name = String::new();
            new_rep_tech.merge_tech_record(tech);
        }
        other_report.tech = new_rep_tech.tech;

        // the very first report is added with minimal changes
        if merge_into.is_none() {
            info!("Adding 1st report (master)");
            other_report.unprocessed_file_names.clear();
            other_report.projects_included.clear();
            merge_into = Some(other_report);
        } else {
            // additional reports are merged
            info!("Merging reports");
            let merge_into_inner = merge_into.as_mut().unwrap();

            // merge all tech records
            for tech in other_report.tech {
                merge_into_inner.merge_tech_record(tech);
            }

            // merge unknown_file_types
            for uft in other_report.file_types {
                merge_into_inner.file_types.increment_counters(uft);
            }

            // collect names of sub-reports in an array for easy retrieval
            if !other_report.report_s3_name.is_empty() {
                merge_into_inner.reports_included.insert(other_report.report_s3_name);
            }

            // update the date of the last commit
            // contributor reports may be missing date_head, but would have last_contributor_commit_date_iso
            let date_head_other = match other_report.date_head.as_ref() {
                Some(v) => Some(v.clone()),
                None => other_report.last_contributor_commit_date_iso.clone(),
            };
            if merge_into_inner.date_head.is_none() {
                merge_into_inner.date_head = date_head_other;
            } else if date_head_other.is_some() {
                // update if the report has a newer date
                if merge_into_inner.date_head.as_ref().unwrap() < date_head_other.as_ref().unwrap() {
                    merge_into_inner.date_head = date_head_other;
                }
            }
            if merge_into_inner.date_head.is_none() {
                // this should not happen - all commits have dates, so should the reports
                warn!("Missing date_head");
            }

            // repeat the same logic for the oldest commit
            // newer contributor reports may contain first_contributor_commit_date_iso,
            // but if that is missing last_contributor_commit_date_iso would do
            let date_init_other = match other_report.date_init.as_ref() {
                Some(v) => Some(v.clone()),
                None => match &other_report.first_contributor_commit_date_iso {
                    Some(v) => Some(v.clone()),
                    None => other_report.last_contributor_commit_date_iso.clone(),
                },
            };
            if merge_into_inner.date_init.is_none() {
                merge_into_inner.date_init = date_init_other;
            } else if date_init_other.is_some() {
                // update if the report has a newer date
                if merge_into_inner.date_init.as_ref().unwrap() > date_init_other.as_ref().unwrap() {
                    merge_into_inner.date_init = date_init_other;
                }
            }
            if merge_into_inner.date_init.is_none() {
                // this should not happen - all commits have dates, so should the reports
                // but date_init was not tracked in contributor reports
                warn!("Missing date_init");
            }

            // update contributor commit dates
            if merge_into_inner.last_contributor_commit_date_iso.is_none() {
                merge_into_inner.last_contributor_commit_date_iso = other_report.last_contributor_commit_date_iso;
            } else if other_report.last_contributor_commit_date_iso.is_some() {
                // update if the other report has a newer date
                if merge_into_inner.last_contributor_commit_date_iso.as_ref().unwrap()
                    < other_report.last_contributor_commit_date_iso.as_ref().unwrap()
                {
                    merge_into_inner.last_contributor_commit_date_iso = other_report.last_contributor_commit_date_iso;
                }
            }
            // repeat the same logic for the 1st commit
            if merge_into_inner.first_contributor_commit_date_iso.is_none() {
                merge_into_inner.first_contributor_commit_date_iso = other_report.first_contributor_commit_date_iso;
            } else if other_report.first_contributor_commit_date_iso.is_some() {
                // update if the other report has a newer date
                if merge_into_inner.first_contributor_commit_date_iso.as_ref().unwrap()
                    > other_report.first_contributor_commit_date_iso.as_ref().unwrap()
                {
                    merge_into_inner.first_contributor_commit_date_iso = other_report.first_contributor_commit_date_iso;
                }
            }

            // add contributor IDs from the other report
            for contributor_git_id in other_report.git_ids_included {
                debug!("Adding git_id: {}", contributor_git_id);
                merge_into_inner.git_ids_included.insert(contributor_git_id);
            }

            // copy the dev identity if the other report is newer by its timestamp
            if other_report.timestamp > merge_into_inner.timestamp {
                merge_into_inner.primary_email = other_report.primary_email;
            }
        }

        // add aggregations based on the merged data
        if let Some(mut report_inner) = merge_into.as_mut() {
            // update the commit time histogram
            CommitTimeHisto::add_commits(&mut report_inner, &other_report_overview.commits);

            // add the project overview
            if report_inner.projects_included.contains(&other_report_overview) {
                // merge matching overviews
                for po in report_inner.projects_included.iter_mut() {
                    if po == &other_report_overview {
                        info!(
                            "Merging ProjOverview for owner:{:?}/{:?}, gh:{:?}/{:?} (into)",
                            po.owner_id, po.project_id, po.github_user_name, po.github_repo_name
                        );
                        info!(
                            "Merging ProjOverview for owner:{:?}/{:?}, gh:{:?}/{:?} (from)",
                            other_report_overview.owner_id,
                            other_report_overview.project_id,
                            other_report_overview.github_user_name,
                            other_report_overview.github_repo_name
                        );
                        po.merge(other_report_overview);
                        break;
                    }
                }
            } else {
                // insert as-is
                report_inner.projects_included.push(other_report_overview);
            }
            // calculate years per technology based on project overview first/last commit
            // it is a temporary crude guess that should be replaced with a proper calculation based on git log --numstats data
            // see https://github.com/stackmuncher/stm_app/issues/46 for more
            report_inner.update_history();
        }

        merge_into
    }

    /// Add a new Tech record merging with the existing records. It removes per-file and some other
    /// potentially sensitive info used for local caching.
    pub(crate) fn merge_tech_record(&mut self, tech: Tech) {
        debug!("Merging Tech, lang: {}, files: {}", tech.language, tech.files);
        // Tech is hashed with the file name for per-file Tech records, but here
        // they are summaries, so it has to be removed to match
        let tech = tech.reset_file_and_commit_info();
        // add totals to the existing record, if any
        if let Some(mut master) = self.tech.take(&tech) {
            debug!("Tech match in master, lang: {}, files: {}", master.language, master.files);
            // add up numeric values
            master.docs_comments += tech.docs_comments;
            master.files += tech.files;
            master.inline_comments += tech.inline_comments;
            master.line_comments += tech.line_comments;
            master.total_lines += tech.total_lines;
            master.blank_lines += tech.blank_lines;
            master.block_comments += tech.block_comments;
            master.bracket_only_lines += tech.bracket_only_lines;
            master.code_lines += tech.code_lines;

            // add keyword counts
            for kw in tech.keywords {
                master.keywords.increment_counters(kw);
            }

            // add dependencies
            for kw in tech.refs {
                master.refs.increment_counters(kw);
            }
            for kw in tech.pkgs {
                master.pkgs.increment_counters(kw);
            }

            // add unique words from dependencies - references
            if tech.refs_kw.is_some() {
                // init the field if None
                if master.refs_kw.is_none() {
                    master.refs_kw = Some(HashSet::new());
                }

                let refs_kw = master.refs_kw.as_mut().unwrap();
                for kw in tech.refs_kw.unwrap() {
                    refs_kw.increment_counters(kw);
                }
            }

            // add unique words from dependencies - packages
            if tech.pkgs_kw.is_some() {
                // init the field if None
                if master.pkgs_kw.is_none() {
                    master.pkgs_kw = Some(HashSet::new());
                }

                let pkgs_kw = master.pkgs_kw.as_mut().unwrap();
                for kw in tech.pkgs_kw.unwrap() {
                    pkgs_kw.increment_counters(kw);
                }
            }

            // re-insert the master record
            self.tech.insert(master);
        } else {
            // there no matching tech record - add it to the hashmap for the 1st time
            // but reset file-specific data first
            debug!("No matching Tech exists - inserting as-is");
            self.tech.insert(tech.reset_file_and_commit_info());
        }
    }

    /// Combines per_file_tech records choosing the most recent record by comparing the commit dates if there is a conflict.
    /// It does not affect `tech` records. They need to be updated using a separate function.
    /// Adds the name of the other report to `reports_included`.
    pub fn merge_same_project_contributor_reports(&mut self, other_report: Self, contributor_git_id: String) {
        debug!("Merging contributor report for {}", contributor_git_id);
        'outer: for tech in other_report.per_file_tech {
            // check if tech should be added to the report at all or is it older than what we already have
            for existing_tech in &self.per_file_tech {
                if *existing_tech == tech && existing_tech.commit_date_epoch > tech.commit_date_epoch {
                    continue 'outer;
                }
            }

            // remove a matching record if it's older
            // this double handling is done because I could not find a way to remove the record inside a for-loop
            self.per_file_tech
                .retain(|t| *t != tech || t.commit_date_epoch > tech.commit_date_epoch);

            // insert the new one
            self.per_file_tech.insert(tech);
        }

        self.git_ids_included.insert(contributor_git_id.clone());

        // merge project metadata - the latest of the two contributor reports gets its data copied over
        // to the combined report
        if let Some(commit_date_iso) = other_report.last_contributor_commit_date_iso {
            if commit_date_iso > self.last_contributor_commit_date_iso.clone().unwrap_or_default() {
                debug!(
                    "Report for {} is newer: {} vs {:?}",
                    contributor_git_id, commit_date_iso, self.last_contributor_commit_date_iso
                );
                self.last_contributor_commit_date_iso = Some(commit_date_iso);
                self.last_contributor_commit_date_epoch = other_report.last_contributor_commit_date_epoch;
                self.last_contributor_commit_sha1 = other_report.last_contributor_commit_sha1;
                self.report_commit_sha1 = other_report.report_commit_sha1;
            }
        }
    }

    /// Deletes existing `tech` records and re-creates them from scratch using `per_file_tech` records.
    pub fn recompute_tech_section(&mut self) {
        debug!("Recomputing tech section");
        self.tech.clear();

        for tech in self.per_file_tech.clone() {
            self.merge_tech_record(tech);
        }
    }

    /// Resets report timestamp, contributor, report IDs and other fields from the individual contributor report
    /// that should not appear in the combined report which may be submitted to the directory.
    /// Adds a list of contributor commits.
    pub fn reset_combined_contributor_report(
        &mut self,
        contributor_git_id: String,
        list_of_commits: &Vec<GitLogEntry>,
        project_report: &Self,
    ) {
        debug!("Resetting combined contributor report for {}", contributor_git_id);
        self.report_id = uuid::Uuid::new_v4().to_string();
        self.timestamp = Utc::now().to_rfc3339();
        self.report_s3_name = String::new();
        self.is_single_commit = false;
        self.log_hash = None;
        self.last_commit_author = None;
        self.git_ids_included.insert(contributor_git_id);
        self.date_head = project_report.date_head.clone();
        self.date_init = project_report.date_init.clone();

        // the latest contributor commit is the first one in the list of commits
        if let Some(latest_log_entry) = list_of_commits.iter().next() {
            self.last_contributor_commit_sha1 = Some(latest_log_entry.sha1.clone());
            self.last_contributor_commit_date_iso = Some(latest_log_entry.date.clone());
            self.last_contributor_commit_date_epoch = Some(latest_log_entry.date_epoch.clone());
        } else {
            warn!("Missing last contributor commit info.");
            self.last_contributor_commit_sha1 = None;
            self.last_contributor_commit_date_iso = None;
            self.last_contributor_commit_date_epoch = None;
        };

        // repeat the same in the reverse order for the very first commit
        if let Some(latest_log_entry) = list_of_commits.iter().last() {
            self.first_contributor_commit_sha1 = Some(latest_log_entry.sha1.clone());
            self.first_contributor_commit_date_iso = Some(latest_log_entry.date.clone());
            self.first_contributor_commit_date_epoch = Some(latest_log_entry.date_epoch.clone());
        } else {
            warn!("Missing last contributor commit info.");
            self.first_contributor_commit_sha1 = None;
            self.first_contributor_commit_date_iso = None;
            self.first_contributor_commit_date_epoch = None;
        };

        // add the list of N recent contributor commits to the project overview and include it into the this combined report
        self.recent_project_commits = Some(
            list_of_commits
                .iter()
                .take(list_of_commits.len().min(500))
                .filter_map(|log_entry| log_entry.join_commit_with_ts())
                .collect(),
        );
    }

    /// Removes some sections that make no sense in the combined report.
    pub fn reset_combined_dev_report<'a>(&mut self) {
        // calculate lengths of lists before they get cleaned up
        debug!("Calculating counts of all report lists");
        let mut list_counts = ListCounts {
            tech: self.tech.len() as u64,
            contributor_git_ids: match &self.contributor_git_ids {
                Some(v) => v.len() as u64,
                None => 0,
            },
            per_file_tech: self.per_file_tech.len() as u64,
            file_types: self.file_types.len() as u64,
            reports_included: self.reports_included.len() as u64,
            projects_included: self.projects_included.len() as u64,
            git_ids_included: self.git_ids_included.len() as u64,
            contributors: match &self.contributors {
                Some(v) => v.len() as u64,
                None => 0,
            },
            tree_files: match &self.tree_files {
                Some(v) => v.len() as u64,
                None => 0,
            },
            recent_project_commits: match &self.recent_project_commits {
                Some(v) => v.len() as u64,
                None => 0,
            },
            keywords: 0,
        };

        self.contributors = None;
        self.tree_files = None;
        self.report_commit_sha1 = None;
        self.last_commit_author = None;
        self.log_hash = None;
        self.commit_count_project = None;
        self.commit_count_contributor = None;
        self.contributor_count = None;
        self.loc_project = None;
        self.libs_project = None;
        self.unprocessed_file_names.clear();
        self.per_file_tech.clear();

        self.github_repo_name = None;
        self.github_user_name = None;
        self.report_id = String::new();
        self.report_s3_name = String::new();
        self.timestamp = Utc::now().to_rfc3339();

        self.recent_project_commits = None;

        // remove lists of commits from project overviews
        for proj in self.projects_included.iter_mut() {
            proj.commits = None;
        }

        // sort project overviews latest to oldest using contributor and head dates, whichever is available
        // ideally this needs to be an impl of PartialOrd trait, but it wouldn't agree with it on ==
        self.projects_included.sort_unstable_by(|a, b| {
            let a = if a.contributor_last_commit.is_some() {
                a.contributor_last_commit.as_ref().unwrap()
            } else if a.date_head.is_some() {
                a.date_head.as_ref().unwrap()
            } else {
                ""
            };

            let b = if b.contributor_last_commit.is_some() {
                b.contributor_last_commit.as_ref().unwrap()
            } else if b.date_head.is_some() {
                b.date_head.as_ref().unwrap()
            } else {
                ""
            };

            b.cmp(a)
        });

        // recalculate commit histograms from absolute number of commits to percentage
        if let Some(histo) = self.commit_time_histo.as_mut() {
            histo.recalculate_counts_to_percentage();
        }

        // extract all keyword into a single container
        self.update_keywords();

        // update the counts with the length of the keywords collection and add it all to the report
        list_counts.keywords = match &self.keywords {
            Some(v) => v.len() as u64,
            None => 0,
        };
        self.list_counts = Some(list_counts);
    }

    /// Returns an abridge copy with some bulky sections removed for indexing in a DB:
    /// * per_file_tech
    /// * contributor.touched_files
    /// * lists of commits
    pub fn abridge(self) -> Self {
        let mut report = self;

        // this can be huge and is not really needed for search
        report.per_file_tech.clear();

        // the list of contributors is useful, but indexing every file in the db isn't needed
        if let Some(contributors) = report.contributors.as_mut() {
            for contributor in contributors {
                contributor.touched_files.clear();
                contributor.commits.clear();
            }
        };

        // commits can be a very long list - they are indexed elsewhere
        report.recent_project_commits = None;
        report.projects_included = report
            .projects_included
            .into_iter()
            .map(|mut pro| {
                pro.commits = None;
                pro
            })
            .collect::<Vec<ProjectReportOverview>>();

        report
    }

    /// Create a blank report with the current timestamp and a unique ID.
    pub(crate) fn new() -> Self {
        Report {
            tech: HashSet::new(),
            per_file_tech: HashSet::new(),
            timestamp: Utc::now().to_rfc3339(),
            unprocessed_file_names: HashSet::new(),
            file_types: HashSet::new(),
            github_user_name: None,
            github_repo_name: None,
            report_s3_name: String::new(),
            report_id: uuid::Uuid::new_v4().to_string(),
            reports_included: HashSet::new(),
            projects_included: Vec::new(),
            git_ids_included: HashSet::new(),
            contributor_git_ids: None,
            contributors: None,
            date_head: None,
            date_init: None,
            tree_files: None,
            report_commit_sha1: None,
            is_single_commit: false,
            log_hash: None,
            last_commit_author: None,
            recent_project_commits: None,
            last_contributor_commit_date_iso: None,
            last_contributor_commit_date_epoch: None,
            last_contributor_commit_sha1: None,
            primary_email: None,
            first_contributor_commit_sha1: None,
            first_contributor_commit_date_iso: None,
            first_contributor_commit_date_epoch: None,
            owner_id: None,
            project_id: None,
            gh_validation_id: None,
            contributor_count: None,
            loc_project: None,
            libs_project: None,
            commit_count_project: None,
            commit_count_contributor: None,
            commit_time_histo: None,
            keywords: None,
            list_counts: None,
        }
    }

    /// Load a report from the local storage, if one exists. Returns None and logs errors on failure.
    pub fn from_disk(path: &PathBuf) -> Option<Self> {
        // check if the file exists at all
        let existing_report_file = Path::new(path);
        if !existing_report_file.exists() {
            info!("No report found at {}. The repo will be processed in full.", path.to_string_lossy());

            return None;
        }

        // try to load the file and read its contents
        let mut existing_report_file = match File::open(path) {
            Err(e) => {
                error!("Cannot read report at {} due to {}.", path.to_string_lossy(), e);
                return None;
            }
            Ok(v) => v,
        };
        let mut report_contents = String::new();
        if let Err(e) = existing_report_file.read_to_string(&mut report_contents) {
            error!("Failed to read report contents from {} due to {}", path.to_string_lossy(), e);
            return None;
        };

        // convert to a struct and return
        match serde_json::from_str::<Report>(&report_contents) {
            Err(e) => {
                error!("Failed to deser report contents from {} due to {}", path.to_string_lossy(), e);
                return None;
            }
            Ok(v) => {
                info!("Loaded a report from {}", path.to_string_lossy());
                return Some(v);
            }
        }
    }

    /// Add a file that won't be processed because it is of unknown type and count the number of files
    /// with the same extension.
    fn add_unprocessed_file(&mut self, file_name: &String) {
        // add the file name to the list
        self.unprocessed_file_names.insert(file_name.clone());
    }

    /// Adds a file extension to a set of counters. Some extensions that look like temp files are excluded.
    pub(crate) fn add_file_type(&mut self, file_name: &String) {
        // check if this particular extension was encountered
        if let Some(position) = file_name.rfind(".") {
            let (_, ext) = file_name.split_at(position);
            let ext = ext.trim_start_matches(".");
            // filter out files with no extension and files that sit in a folder
            // starting with a ., e.g. `.bin/license`
            // files starting or ending with _ are usually of no interest
            if ext.len() == 0 || ext.len() > 20 || ext.starts_with("_") || ext.ends_with("_") {
                debug!("Invalid ext: {}", ext);
                return;
            }

            // validate every char in the extension - only alphanumerics, _, - are allowed
            for chr in ext.chars() {
                if chr == '-' || chr == '_' || chr.is_ascii_alphanumeric() {
                    // that's a valid char, let it be
                } else {
                    debug!("Invalid ext: {}", ext);
                    return;
                }
            }

            let ext = KeywordCounter {
                k: ext.trim_start_matches(".").to_string(),
                t: None,
                c: 1,
            };
            self.file_types.increment_counters(ext);
        }
    }

    /// Serializes the report and saves it in the specified location. Panics if either serialize or save fail.
    /// Prettified reports can be twice as big as non-formatted ones. Only use this option for reports that the user may want to look at.
    pub fn save_as_local_file(&self, file_name: &PathBuf, make_pretty: bool) {
        let absolute_file_name = file_name
            .absolutize()
            .expect("Cannot convert rules / file_type dir path to absolute. It's a bug.")
            .to_path_buf();

        // choose the json serializer (pretty or compressed)
        let to_json = if make_pretty {
            |a: &Self| serde_json::to_vec_pretty(a)
        } else {
            |a: &Self| serde_json::to_vec(a)
        };

        // serialize the report into bytes
        let payload = match to_json(&self) {
            Err(e) => {
                error!("Cannot save a report in {} due to {}", absolute_file_name.to_string_lossy(), e);
                std::process::exit(1);
            }
            Ok(v) => v,
        };

        // save into a file
        if let Err(e) = std::fs::write(file_name, payload.clone()) {
            error!("Cannot save a report in {} due to {}", absolute_file_name.to_string_lossy(), e);
            std::process::exit(1);
        };

        info!("Report saved into {}", absolute_file_name.to_string_lossy());
    }

    /// Adds details about the commit history to the report: head, init, contributors, collaborators, log hash, and remote URLs.
    /// Does not panic (exits early) if `git rev-list` command fails.
    pub(crate) async fn add_commits_history(self, git_log: Vec<GitLogEntry>) -> Self {
        let mut report = self;
        debug!("Adding commit history");

        report.commit_count_project = Some(git_log.len() as u64);

        // get the date of the last commit
        if let Some(commit) = git_log.iter().next() {
            if commit.date_epoch > 0 {
                report.date_head = Some(commit.date.clone());
                report.report_commit_sha1 = Some(commit.sha1.clone());
                report.last_commit_author =
                    Some(Contributor::git_identity_from_name_email_pair(&commit.author_name_email));
            }
        }

        // get the date of the first commit
        if let Some(commit) = git_log.iter().last() {
            if commit.date_epoch > 0 {
                report.date_init = Some(commit.date.clone());
            }
        }

        // hash the list of commits to determine if there were any history re-writes
        report.log_hash = Some(utils::hash_vec_sha1(
            git_log.iter().map(|entry| entry.sha1.clone()).collect::<Vec<String>>(),
        ));

        // compile a list of all project commits for matching forks and clones
        // the SHA1 is truncated to 8 chars to save space, but it increases the chance of collision
        // https://github.com/source-foundry/font-v/issues/2
        report.recent_project_commits = Some(
            git_log
                .iter()
                .filter_map(|log_entry| log_entry.join_commit_with_ts())
                .collect::<Vec<String>>(),
        );

        // this part consumes git_log because there is a lot of data in it
        // so should appear at the end
        report.contributors = Some(Contributor::from_commit_history(git_log));
        report.contributor_git_ids = Some(
            report
                .contributors
                .as_ref()
                .unwrap()
                .iter()
                .map(|contributor| contributor.git_id.clone())
                .collect::<HashSet<String>>(),
        );
        report.contributor_count = Some(
            report
                .contributors
                .as_ref()
                .expect("Cannot unwrap report.contributor_git_ids. It's a bug")
                .len() as u64,
        );

        report
    }

    /// Copy the list of collaborators, init and head dates from the old report.
    pub async fn copy_commit_info(self, old_report: &Self) -> Self {
        let mut report = self;

        report.contributors = old_report.contributors.clone();
        report.date_head = old_report.date_head.clone();
        report.date_init = old_report.date_init.clone();
        info!("Copied commit info from the old report");

        report
    }

    /// Adds the entire list of tree files or just the touched files to the report, extracts names of unprocessed files
    /// and counts their extensions.
    pub fn update_project_file_lists(self, all_tree_files: HashSet<String>) -> Self {
        // result collector
        let mut report = self;

        // subtract processed files from all files to get the list of unprocessed files
        let processed_files = report
            .per_file_tech
            .iter()
            .map(|tech| tech.file_name.as_ref().unwrap_or(&String::new()).clone())
            .collect::<HashSet<String>>();
        let unprocessed_files = all_tree_files
            .difference(&processed_files)
            .map(|f| f)
            .collect::<Vec<&String>>();

        // store the names of unprocessed files in the report
        debug!("Found {} un-processed files", unprocessed_files.len());
        for f in unprocessed_files {
            report.add_unprocessed_file(f);
        }

        // get total counts per file extension
        for file_name in &all_tree_files {
            report.add_file_type(file_name);
        }

        // save the entire list of tree files in the report
        report.tree_files = Some(all_tree_files);

        report
    }

    /// Removes or replaces any sensitive info from the report for submission to stackmuncher.com.
    /// Requires a `salt` for name hashing. It has to be unique to the user, consistent across submissions, but is only known to the user
    pub fn sanitize(&self, salt: String) -> Result<Self, ()> {
        // this function should be replaced with a macro
        // see https://github.com/stackmuncher/stm_app/issues/12

        info!("Report pre-submission cleanup started");
        // expensive, but probably unavoidable given that the original report will still be used at the point of call
        let mut report = self.clone();

        // clean up per_file_tech section
        let per_file_tech = report.per_file_tech.drain().collect::<Vec<Tech>>();
        for mut x in per_file_tech {
            // use a signed public key as the salt to make the file name hash consistent across submissions by the same user
            // making it very hard to match them across different users
            // it would be computationally prohibitive to try and find a match,
            x.file_name =
                Some(hash_str_to_sha256_as_base58(&[&salt, x.file_name.unwrap_or_default().as_str()].concat()));
            x.keywords.clear();
            x.pkgs.clear();
            x.pkgs_kw = None;
            x.refs.clear();
            x.refs_kw = None;
            report.per_file_tech.insert(x);
        }

        // this may be an email address of someone else
        report.last_commit_author = None;
        // someone's else commit hash can be used for matching across devs
        report.report_commit_sha1 = None;

        // reset time component of the project head and init commit timestamps to prevent cross-developer project matching
        if let Some(date_head) = &report.date_head {
            match DateTime::parse_from_rfc3339(date_head) {
                Err(e) => {
                    warn!("Invalid HEAD commit date: {} ({}). Expected RFC3339 format.", date_head, e);
                    report.date_head = None;
                }
                Ok(v) => {
                    report.date_head = Some(v.date().and_hms(0, 0, 0).to_rfc3339());
                }
            }
        }

        if let Some(date_init) = &report.date_init {
            match DateTime::parse_from_rfc3339(date_init) {
                Err(e) => {
                    warn!("Invalid INIT commit date: {} ({}). Expected RFC3339 format.", date_init, e);
                    report.date_init = None;
                }
                Ok(v) => {
                    report.date_init = Some(v.date().and_hms(0, 0, 0).to_rfc3339());
                }
            }
        }

        Ok(report)
    }

    /// GZips itself
    pub fn gzip(&self) -> Result<Vec<u8>, ()> {
        // serialize the report into bytes
        let report = match serde_json::to_vec(&self) {
            Err(e) => {
                error!("Cannot serialize a report after pre-sub cleanup due to {}", e);
                return Err(());
            }
            Ok(v) => v,
        };

        // gzip it
        let mut encoder = GzEncoder::new(Vec::new(), Compression::default());
        if let Err(e) = encoder.write_all(&report) {
            error!("Cannot gzip the report due to {}", e);
            return Err(());
        };
        let gzip_bytes = match encoder.finish() {
            Err(e) => {
                error!("Cannot finish gzipping the report due to {}", e);
                return Err(());
            }

            Ok(v) => v,
        };

        info!("Report size: {}, GZip: {}", report.len(), gzip_bytes.len());

        Ok(gzip_bytes)
    }

    /// Updates itself with total counts for `loc_project` and `libs_project`.
    pub(crate) fn with_summary(self) -> Self {
        // collect summary
        let loc_project = Some(self.tech.iter().map(|t| t.code_lines).sum::<u64>());
        let libs_project = Some(
            self.tech
                .iter()
                .map(|t| t.refs.len() as u64 + t.pkgs.len() as u64)
                .sum::<u64>(),
        );

        Self {
            loc_project,
            libs_project,
            ..self
        }
    }

    /// Parses `self.timestamp` from RFC3339 to an EPOCH. Returns 0 if the value is not valid.
    pub fn parsed_timestamp(&self) -> i64 {
        // check if the report is in an older format and has to be reprocessed regardless
        if let Ok(ts) = DateTime::parse_from_rfc3339(&self.timestamp) {
            ts.with_timezone(&Utc).timestamp()
        } else {
            warn!("Invalid cached report timestamp: {}", self.timestamp);
            0
        }
    }

    /// Parses `REPORT_FORMAT_VERSION` from RFC3339 to an EPOCH. Panics if the value is not valid.
    /// Reports produced prior to that date should be updated.
    pub fn report_format_version() -> i64 {
        // check if the report is in an older format and has to be reprocessed regardless
        DateTime::parse_from_rfc3339(Report::REPORT_FORMAT_VERSION)
            .expect("Invalid REPORT_FORMAT_VERSION value.")
            .timestamp()
    }

    /// Returns TRUE if the report is in an older format than the current version.
    pub fn is_outdated_format(&self) -> bool {
        self.parsed_timestamp() < Report::report_format_version()
    }

    /// Updated `keywords` member from all `refs` and `pkgs`. Splits words at separators like _-/@
    pub(crate) fn update_keywords(&mut self) {
        // create a new keywords container if none exists
        if self.keywords.is_none() {
            self.keywords = Some(HashSet::new());
        }

        // loop through all refs and packages adding them as keywords after normalizing
        if let Some(keywords) = self.keywords.as_mut() {
            for tech in &self.tech {
                if let Some(pkgs) = &tech.pkgs_kw {
                    for kwc in pkgs {
                        for kw in kwc.split() {
                            keywords.insert(kw);
                        }
                    }
                }
                // repeat the same step for keywords
                if let Some(pkgs) = &tech.refs_kw {
                    for kwc in pkgs {
                        for kw in kwc.split() {
                            keywords.insert(kw);
                        }
                    }
                }
            }
        }

        // remove the container if it's empty
        if self.keywords.as_ref().unwrap().is_empty() {
            self.keywords = None;
        }
    }

    /// Updates all tech/history records with a summary from other parts of the report.
    pub(crate) fn update_history(&mut self) {
        // calculate total years per tech from project overviews
        let mut tech_history_map: HashMap<&String, TechHistory> = HashMap::new();

        for project_overview in &self.projects_included {
            // get from and to dates from possible sources
            // skip to the next project overview if no data can be found
            let date_from = match &project_overview.contributor_first_commit {
                Some(v) => v,
                None => match &project_overview.date_init {
                    Some(v) => v,
                    None => {
                        warn!("No 1st commit or repo init date in {}", project_overview.project_name);
                        continue;
                    }
                },
            };
            let date_to = match &project_overview.contributor_last_commit {
                Some(v) => v,
                None => match &project_overview.date_head {
                    Some(v) => v,
                    None => {
                        warn!("No last commit or repo head date in {}", project_overview.project_name);
                        continue;
                    }
                },
            };

            // convert strings into DateTime
            let date_from = match DateTime::parse_from_rfc3339(&date_from) {
                Err(e) => {
                    error!("Invalid 1st or init commit date: {} ({}). Expected RFC3339 format.", date_from, e);
                    continue;
                }
                Ok(v) => v.date().and_hms(0, 0, 0).with_timezone(&Utc),
            };
            let date_to = match DateTime::parse_from_rfc3339(&date_to) {
                Err(e) => {
                    error!("Invalid last or HEAD commit date: {} ({}). Expected RFC3339 format.", date_to, e);
                    continue;
                }
                Ok(v) => v.date().and_hms(0, 0, 0).with_timezone(&Utc),
            };

            // loop through every tech in the project overview to update from/to ranges
            for tech in &project_overview.tech {
                // get a reference to the existing tech history
                let tech_history = match tech_history_map.get_mut(&tech.language) {
                    Some(v) => v,
                    None => {
                        // this is the first time this tech is encountered - create a new struct, push into HashMap and continue to the next tech
                        let th = TechHistory {
                            months: 0,
                            from_date_epoch: date_from.timestamp(),
                            from_date_iso: date_from.to_rfc3339(),
                            to_date_epoch: date_to.timestamp(),
                            to_date_iso: date_to.to_rfc3339(),
                        };
                        tech_history_map.insert(&tech.language, th);
                        continue;
                    }
                };

                // update the date range in the tech history container
                if tech_history.from_date_epoch > date_from.timestamp() {
                    tech_history.from_date_epoch = date_from.timestamp();
                    tech_history.from_date_iso = date_from.to_rfc3339()
                }
                if tech_history.to_date_epoch < date_to.timestamp() {
                    tech_history.to_date_epoch = date_to.timestamp();
                    tech_history.to_date_iso = date_to.to_rfc3339()
                }
            }
        }

        // update duration for every tech
        for (_, tech) in tech_history_map.iter_mut() {
            // calculated as 31_536_000 / 12
            // this i64 -> u64 conversion should be safe and is needed if somehow the dates are reversed and the result is negative
            // it will simply produce a zero
            tech.months = (tech.to_date_epoch - tech.from_date_epoch).max(0) as u64 / 2_628_000;
        }

        // place the tech histories into the top level tech records of the report
        self.tech = self
            .tech
            .drain()
            .map(|mut v| {
                v.history = tech_history_map.remove(&v.language);
                v
            })
            .collect::<HashSet<Tech>>();
    }
}

impl std::fmt::Display for Report {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match serde_json::to_string(self) {
            Ok(v) => {
                write!(f, "{}", v).expect("Invalid JSON string in report.");
            }
            Err(e) => {
                write!(f, "Cannot serialize Report {:?}", e).expect("Invalid error msg in report.");
            }
        }
        Ok(())
    }
}

#[cfg(test)]
mod test_report {
    use super::Report;
    use std::fs::File;
    use std::io::prelude::*;

    #[test]
    fn test_merge() {
        tracing_subscriber::fmt()
            .with_max_level(tracing::Level::TRACE)
            .with_ansi(false)
            .init();

        let r1 = File::open("test-files/report1.json").unwrap();
        let r1: Report = serde_json::from_reader(r1).unwrap();

        let r2 = File::open("test-files/report2.json").unwrap();
        let r2: Report = serde_json::from_reader(r2).unwrap();

        // calculate the expected sums of files
        let cs_files: u64 = r1
            .tech
            .iter()
            .chain(r2.tech.iter())
            .map(|t| if t.language == "C#" { t.files } else { 0 })
            .sum();
        let md_files: u64 = r1
            .tech
            .iter()
            .chain(r2.tech.iter())
            .map(|t| if t.language == "Markdown" { t.files } else { 0 })
            .sum();
        let ps1_files: u64 = r1
            .tech
            .iter()
            .chain(r2.tech.iter())
            .map(|t| if t.language == "PowerShell" { t.files } else { 0 })
            .sum();

        // do the same for refs and pkgs in C#
        let cs_refs: u64 = r1
            .tech
            .iter()
            .chain(r2.tech.iter())
            .map(|t| {
                if t.language == "C#" {
                    let rs: u64 = t.refs.iter().map(|tr| tr.c).sum();
                    rs
                } else {
                    0
                }
            })
            .sum();
        let cs_pkgs: u64 = r1
            .tech
            .iter()
            .chain(r2.tech.iter())
            .map(|t| {
                if t.language == "C#" {
                    let rs: u64 = t.pkgs.iter().map(|tr| tr.c).sum();
                    rs
                } else {
                    0
                }
            })
            .sum();

        let rm = Report::merge(None, r1).unwrap();
        let rm = Report::merge(Some(rm), r2).unwrap();
        let rms = serde_json::to_string_pretty(&rm).unwrap();

        let mut rmf = File::create("test-files/report_merged.json").unwrap();
        let _ = rmf.write_all(&mut rms.as_bytes());

        // compare number of files
        for t in rm.tech.iter() {
            match t.language.as_str() {
                "C#" => {
                    assert_eq!(t.files, cs_files, "C# file count");
                }
                "Markdown" => {
                    assert_eq!(t.files, md_files, "Markdown file count");
                }
                "PowerShell" => {
                    assert_eq!(t.files, ps1_files, "PowerShell file count");
                }
                _ => assert!(false, "Unexpected language {}", t.language),
            }
        }

        // compare number of refs and pkgs for C#
        let cs_refs_rm: u64 = rm
            .tech
            .iter()
            .map(|t| {
                if t.language == "C#" {
                    let rs: u64 = t.refs.iter().map(|tr| tr.c).sum();
                    rs
                } else {
                    0
                }
            })
            .sum();
        println!("Refs counts, merged: {}, expected {}", cs_refs_rm, cs_refs);
        assert_eq!(cs_refs_rm, cs_refs, "C# refs count");

        let cs_pkgs_rm: u64 = rm
            .tech
            .iter()
            .map(|t| {
                if t.language == "C#" {
                    let rs: u64 = t.pkgs.iter().map(|tr| tr.c).sum();
                    rs
                } else {
                    0
                }
            })
            .sum();
        println!("Pkgs counts, merged: {}, expected {}", cs_pkgs_rm, cs_pkgs);
        assert_eq!(cs_pkgs_rm, cs_pkgs, "C# pkgs count");
    }
}
